{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. [Load and transform the Adults dataset](#1)\n",
    "2. [Create a model](#2)\n",
    "3. [Create and Train the ART classifier](#3)\n",
    "4. [Generate adversarial test examples](#4)\n",
    "5. [Evaluate the model on benign and adversarial samples](#5)\n",
    "    - [5.1 On Training Set](#5_1)\n",
    "    - [5.2 On Test Set](#5_2)\n",
    "    \n",
    "- **Result**\n",
    "    - Training score **100%** on Benign, **49%** on adversarial samples\n",
    "    - Testing score **81%** on Benign, **37%** on adversarial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from metric import evaluate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "**1. Load and transform the Adults dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_adults\n",
    "\n",
    "# Step 1: Load the Adults dataset\n",
    "(X_train, y_train), (X_test, y_test) = load_adults(True)\n",
    "#X_train.shape (32561, 14)\n",
    "#X_test.shape (16281, 14)\n",
    "#y_train.shape (32561,)\n",
    "#y_test.shape (16281,)\n",
    "\n",
    "n_train = 100\n",
    "n_test = 75\n",
    "X_train = X_train[0:n_train]\n",
    "y_train = y_train[0:n_train]\n",
    "X_test = X_test[0:n_test]\n",
    "y_test = y_test[0:n_test]\n",
    "#X_train.shape (100, 14)\n",
    "#y_train.shape (100, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "**2. Create the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the model\n",
    "model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                               random_state=None, max_leaf_nodes=50, min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "# Create the model: SVC\n",
    "# from sklearn.svm import SVC\n",
    "# model = SVC(gamma='scale', probability=True, max_iter = -1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "**3. Create and Train the ART classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import SklearnClassifier\n",
    "\n",
    "# Create the ART classifier\n",
    "classifier = SklearnClassifier(model=model)\n",
    "\n",
    "# Train the ART classifier\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "**4. Generate adversarial test examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import DecisionTreeAttack\n",
    "\n",
    "# Create ART attack\n",
    "attack = DecisionTreeAttack(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decision tree attack: 100%|█████████████████████████████████| 100/100 [00:00<00:00, 354.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate adversarial samples\n",
    "X_train_adv = attack.generate(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decision tree attack: 100%|███████████████████████████████████| 75/75 [00:00<00:00, 833.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate adversarial samples\n",
    "X_test_adv = attack.generate(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "**5. Evaluate the model on benign and adversarial samples**\n",
    "- [5.1 On Training Set](#5_1)\n",
    "- [5.2 On Test Set](#5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5_1\"></a>\n",
    "- 5.1. Evaluate the model on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign training:\n",
      "y_pred: [0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0]\n",
      "Misclassified examples:0\n",
      "confusion matrix:\n",
      " [[75  0]\n",
      " [ 0 25]]\n",
      "accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Benign training:\")\n",
    "evaluate(classifier, X_train, y_train) #benign sample\n",
    "# prediction = classifier.predict(X_train[0:1])[0]\n",
    "# print(\"Benign Training Predicted Label: %i\" % np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training:\n",
      "y_pred: [1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
      " 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1]\n",
      "Misclassified examples:51\n",
      "confusion matrix:\n",
      " [[36 39]\n",
      " [12 13]]\n",
      "accuracy: 0.49\n",
      "Precision: 0.25\n",
      "Recall: 0.52\n",
      "F1: 0.33766233766233766\n",
      "auc: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adversarial training:\")\n",
    "evaluate(classifier, X_train_adv, y_train) #Adversarial sample\n",
    "# prediction = classifier.predict(X_train_adv[0:1, :])[0]\n",
    "# print(\"Adversarial Training Predicted Label: %i\"  % np.argmax(prediction))\n",
    "# #prediction [1., 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5_2\"></a>\n",
    "- 5.2. Evaluate the model on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign testing:\n",
      "y_pred: [0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1]\n",
      "Misclassified examples:14\n",
      "confusion matrix:\n",
      " [[49  7]\n",
      " [ 7 12]]\n",
      "accuracy: 0.8133333333333334\n",
      "Precision: 0.631578947368421\n",
      "Recall: 0.631578947368421\n",
      "F1: 0.631578947368421\n",
      "auc: 0.7532894736842105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Benign testing:\")\n",
    "evaluate(classifier, X_test, y_test) #benign sample\n",
    "# prediction = classifier.predict(X_test[0:1])[0]\n",
    "# print(\"Benign Testing Predicted Label: %i\" % np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial testing:\n",
      "y_pred: [0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1\n",
      " 1]\n",
      "Misclassified examples:47\n",
      "confusion matrix:\n",
      " [[25 31]\n",
      " [16  3]]\n",
      "accuracy: 0.37333333333333335\n",
      "Precision: 0.08823529411764706\n",
      "Recall: 0.15789473684210525\n",
      "F1: 0.11320754716981131\n",
      "auc: 0.30216165413533835\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adversarial testing:\")\n",
    "evaluate(classifier, X_test_adv, y_test) #adversarial sample\n",
    "# prediction = classifier.predict(X_test_adv[0:1])[0]\n",
    "# print(\"Adversarial Training Predicted Label: %i\"  % np.argmax(prediction))\n",
    "# #prediction [1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
